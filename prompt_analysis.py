#!/usr/bin/env python3
"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ†æãƒ»åˆ†è§£ãƒ»ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºæ¤œè¨¼ãƒ„ãƒ¼ãƒ«

åé›†ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€åŠ¹æœçš„ãªåˆ†è§£ãƒ»åˆ†é¡æˆ¦ç•¥ã‚’æ¤œè¨ã™ã‚‹
"""

import sqlite3
import re
import json
from collections import Counter, defaultdict
from typing import List, Dict, Set, Tuple
import pandas as pd

class PromptAnalyzer:
    def __init__(self, db_path: str = "data/civitai_dataset.db"):
        self.db_path = db_path
        self.conn = None

    def connect_db(self):
        self.conn = sqlite3.connect(self.db_path)

    def close_db(self):
        if self.conn:
            self.conn.close()

    def analyze_prompt_structure(self) -> Dict:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ ã®åŸºæœ¬åˆ†æ"""
        self.connect_db()
        cursor = self.conn.cursor()

        # åŸºæœ¬çµ±è¨ˆ
        cursor.execute("""
            SELECT
                COUNT(*) as total_prompts,
                AVG(prompt_length) as avg_length,
                MIN(prompt_length) as min_length,
                MAX(prompt_length) as max_length,
                AVG(tag_count) as avg_tag_count,
                MIN(tag_count) as min_tag_count,
                MAX(tag_count) as max_tag_count
            FROM civitai_prompts
        """)

        stats = cursor.fetchone()

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—
        cursor.execute("SELECT full_prompt, negative_prompt FROM civitai_prompts LIMIT 10")
        samples = cursor.fetchall()

        self.close_db()

        return {
            'total_prompts': stats[0],
            'avg_length': stats[1],
            'min_length': stats[2],
            'max_length': stats[3],
            'avg_tag_count': stats[4],
            'min_tag_count': stats[5],
            'max_tag_count': stats[6],
            'samples': samples
        }

    def extract_common_patterns(self) -> Dict:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        self.connect_db()
        cursor = self.conn.cursor()

        cursor.execute("SELECT full_prompt FROM civitai_prompts WHERE full_prompt IS NOT NULL")
        prompts = [row[0] for row in cursor.fetchall()]

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns = {
            'quality_terms': Counter(),
            'style_terms': Counter(),
            'character_terms': Counter(),
            'technical_terms': Counter(),
            'comma_separated': 0,
            'parentheses_usage': 0,
            'weight_usage': 0,
            'embedding_usage': 0
        }

        # å“è³ªé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        quality_keywords = ['masterpiece', 'best quality', 'high quality', 'extremely detailed',
                           'ultra detailed', 'detailed', 'realistic', 'photorealistic',
                           'high resolution', '8k', '4k', 'ultra high res']

        # ã‚¹ã‚¿ã‚¤ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        style_keywords = ['anime', 'realistic', 'cartoon', 'oil painting', 'watercolor',
                         'digital art', 'concept art', 'portrait', 'landscape', 'abstract']

        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        character_keywords = ['girl', 'boy', 'woman', 'man', 'female', 'male', 'person',
                            'beautiful', 'cute', 'handsome', 'young', 'old']

        # æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        technical_keywords = ['depth of field', 'bokeh', 'lighting', 'shadows', 'composition',
                            'camera angle', 'perspective', 'focus', 'blur', 'sharp']

        for prompt in prompts:
            if not prompt:
                continue

            prompt_lower = prompt.lower()

            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒã‚§ãƒƒã‚¯
            if ',' in prompt:
                patterns['comma_separated'] += 1

            # æ‹¬å¼§ä½¿ç”¨ãƒã‚§ãƒƒã‚¯
            if '(' in prompt or '[' in prompt:
                patterns['parentheses_usage'] += 1

            # é‡ã¿ä»˜ã‘ãƒã‚§ãƒƒã‚¯ (:æ•°å­—)
            if re.search(r':\s*\d+\.?\d*', prompt):
                patterns['weight_usage'] += 1

            # ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨ãƒã‚§ãƒƒã‚¯
            if '<' in prompt and '>' in prompt:
                patterns['embedding_usage'] += 1

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
            for keyword in quality_keywords:
                if keyword in prompt_lower:
                    patterns['quality_terms'][keyword] += 1

            for keyword in style_keywords:
                if keyword in prompt_lower:
                    patterns['style_terms'][keyword] += 1

            for keyword in character_keywords:
                if keyword in prompt_lower:
                    patterns['character_terms'][keyword] += 1

            for keyword in technical_keywords:
                if keyword in prompt_lower:
                    patterns['technical_terms'][keyword] += 1

        self.close_db()
        return patterns

    def categorize_prompts(self) -> Dict:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è‡ªå‹•ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºè©¦è¡Œ"""
        self.connect_db()
        cursor = self.conn.cursor()

        cursor.execute("SELECT id, full_prompt, quality_score FROM civitai_prompts WHERE full_prompt IS NOT NULL")
        prompts = cursor.fetchall()

        categories = {
            'realistic_portrait': [],
            'anime_character': [],
            'landscape_scene': [],
            'abstract_art': [],
            'technical_photo': [],
            'fantasy_creature': [],
            'uncategorized': []
        }

        # ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºãƒ«ãƒ¼ãƒ«
        for pid, prompt, quality in prompts:
            if not prompt:
                continue

            prompt_lower = prompt.lower()

            # ãƒªã‚¢ãƒ«ãƒãƒ¼ãƒˆãƒ¬ãƒ¼ãƒˆ
            if any(word in prompt_lower for word in ['realistic', 'photorealistic', 'portrait', 'woman', 'man', 'girl', 'boy']):
                if any(word in prompt_lower for word in ['realistic', 'photorealistic', 'photo']):
                    categories['realistic_portrait'].append((pid, prompt[:100], quality))
                    continue

            # ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼
            if any(word in prompt_lower for word in ['anime', 'manga', 'cartoon', '1girl', '1boy', 'cute']):
                categories['anime_character'].append((pid, prompt[:100], quality))
                continue

            # é¢¨æ™¯ã‚·ãƒ¼ãƒ³
            if any(word in prompt_lower for word in ['landscape', 'scenery', 'background', 'environment', 'nature']):
                categories['landscape_scene'].append((pid, prompt[:100], quality))
                continue

            # æŠ½è±¡ã‚¢ãƒ¼ãƒˆ
            if any(word in prompt_lower for word in ['abstract', 'artistic', 'concept art', 'surreal']):
                categories['abstract_art'].append((pid, prompt[:100], quality))
                continue

            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«å†™çœŸ
            if any(word in prompt_lower for word in ['macro', 'close-up', 'depth of field', 'bokeh', 'professional']):
                categories['technical_photo'].append((pid, prompt[:100], quality))
                continue

            # ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼
            if any(word in prompt_lower for word in ['fantasy', 'dragon', 'magic', 'creature', 'monster']):
                categories['fantasy_creature'].append((pid, prompt[:100], quality))
                continue

            # æœªåˆ†é¡
            categories['uncategorized'].append((pid, prompt[:100], quality))

        self.close_db()
        return categories

    def suggest_tag_taxonomy(self) -> Dict:
        """ã‚¿ã‚°åˆ†é¡ä½“ç³»ã®ææ¡ˆ"""
        patterns = self.extract_common_patterns()

        taxonomy = {
            'quality_modifiers': {
                'description': 'å“è³ªãƒ»è§£åƒåº¦é–¢é€£ã®ã‚¿ã‚°',
                'examples': list(patterns['quality_terms'].most_common(10)),
                'usage': 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªå‘ä¸Šã«ä½¿ç”¨'
            },
            'style_descriptors': {
                'description': 'ã‚¢ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ãƒ»è¡¨ç¾æ–¹æ³•',
                'examples': list(patterns['style_terms'].most_common(10)),
                'usage': 'å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«ã®æŒ‡å®š'
            },
            'subject_elements': {
                'description': 'ä¸»é¡Œãƒ»ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¦ç´ ',
                'examples': list(patterns['character_terms'].most_common(10)),
                'usage': 'æç”»å¯¾è±¡ã®æŒ‡å®š'
            },
            'technical_parameters': {
                'description': 'æŠ€è¡“çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿',
                'examples': list(patterns['technical_terms'].most_common(10)),
                'usage': 'ã‚«ãƒ¡ãƒ©è¨­å®šã‚„æŠ€è¡“çš„åŠ¹æœ'
            },
            'syntax_patterns': {
                'comma_separation': patterns['comma_separated'],
                'weight_notation': patterns['weight_usage'],
                'embedding_usage': patterns['embedding_usage'],
                'parentheses_usage': patterns['parentheses_usage']
            }
        }

        return taxonomy

def main():
    """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
    analyzer = PromptAnalyzer()

    print("ğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹...")
    print("=" * 60)

    # 1. åŸºæœ¬æ§‹é€ åˆ†æ
    print("\nğŸ“Š 1. åŸºæœ¬æ§‹é€ åˆ†æ")
    stats = analyzer.analyze_prompt_structure()
    print(f"ç·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {stats['total_prompts']}")
    print(f"å¹³å‡æ–‡å­—æ•°: {stats['avg_length']:.1f}")
    print(f"æ–‡å­—æ•°ç¯„å›²: {stats['min_length']} - {stats['max_length']}")
    print(f"å¹³å‡ã‚¿ã‚°æ•°: {stats['avg_tag_count']:.1f}")
    print(f"ã‚¿ã‚°æ•°ç¯„å›²: {stats['min_tag_count']} - {stats['max_tag_count']}")

    # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    print("\nğŸ¯ 2. å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
    patterns = analyzer.extract_common_patterns()

    print(f"ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šä½¿ç”¨: {patterns['comma_separated']}/{stats['total_prompts']} ({patterns['comma_separated']/stats['total_prompts']*100:.1f}%)")
    print(f"é‡ã¿ä»˜ã‘ä½¿ç”¨: {patterns['weight_usage']}/{stats['total_prompts']} ({patterns['weight_usage']/stats['total_prompts']*100:.1f}%)")
    print(f"æ‹¬å¼§ä½¿ç”¨: {patterns['parentheses_usage']}/{stats['total_prompts']} ({patterns['parentheses_usage']/stats['total_prompts']*100:.1f}%)")
    print(f"ã‚¨ãƒ³ãƒ™ãƒƒãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨: {patterns['embedding_usage']}/{stats['total_prompts']} ({patterns['embedding_usage']/stats['total_prompts']*100:.1f}%)")

    print("\nğŸ† å“è³ªé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP5:")
    for keyword, count in patterns['quality_terms'].most_common(5):
        print(f"  {keyword}: {count}å›")

    print("\nğŸ¨ ã‚¹ã‚¿ã‚¤ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP5:")
    for keyword, count in patterns['style_terms'].most_common(5):
        print(f"  {keyword}: {count}å›")

    # 3. ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºè©¦è¡Œ
    print("\nğŸ“‚ 3. è‡ªå‹•ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºçµæœ")
    categories = analyzer.categorize_prompts()

    for category, prompts in categories.items():
        if prompts:
            print(f"\n{category.replace('_', ' ').title()}: {len(prompts)}ä»¶")
            # ä¸Šä½3ä»¶ã‚’è¡¨ç¤º
            for i, (pid, prompt, quality) in enumerate(prompts[:3]):
                print(f"  [{pid}] (Q:{quality}) {prompt}...")

    # 4. ã‚¿ã‚°åˆ†é¡ä½“ç³»ææ¡ˆ
    print("\nğŸ—ï¸ 4. æ¨å¥¨ã‚¿ã‚°åˆ†é¡ä½“ç³»")
    taxonomy = analyzer.suggest_tag_taxonomy()

    for category, info in taxonomy.items():
        if category != 'syntax_patterns':
            print(f"\n{category.replace('_', ' ').title()}:")
            print(f"  èª¬æ˜: {info['description']}")
            print(f"  ç”¨é€”: {info['usage']}")
            print(f"  ä¸»è¦ä¾‹: {[item[0] for item in info['examples'][:5]]}")

    print("\nğŸ“ æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ä½¿ç”¨çŠ¶æ³:")
    syntax = taxonomy['syntax_patterns']
    for pattern, count in syntax.items():
        if pattern != 'syntax_patterns':
            print(f"  {pattern.replace('_', ' ')}: {count}")

    print("\nâœ… åˆ†æå®Œäº†ï¼")
    print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
    print("1. ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå½¢å¼ãŒä¸»æµ â†’ ãƒ‘ãƒ¼ã‚µãƒ¼ã¯ã‚«ãƒ³ãƒåˆ†å‰²ãƒ™ãƒ¼ã‚¹")
    print("2. é‡ã¿ä»˜ã‘è¨˜æ³•ã‚’è€ƒæ…® â†’ :æ•°å€¤ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡¦ç†")
    print("3. å“è³ªã‚¿ã‚°ã¯å…±é€š â†’ æ¨™æº–åŒ–ã—ã¦å†åˆ©ç”¨å¯èƒ½")
    print("4. ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é¡ã¯æ˜ç¢º â†’ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆ")
    print("5. æŠ€è¡“çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯åˆ†é›¢ â†’ ComfyUIè¨­å®šã¨ã®é€£æº")

if __name__ == "__main__":
    main()
