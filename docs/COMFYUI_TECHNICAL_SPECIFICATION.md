# ComfyUI CivitAI Nodes æŠ€è¡“ä»•æ§˜æ›¸

## 1. ãƒãƒ¼ãƒ‰å®Ÿè£…è©³ç´°ä¾‹

### CivitaiDataCollectorNode å®Œå…¨å®Ÿè£…ä¾‹

```python
# nodes/civitai_collector_node.py

import asyncio
import threading
from typing import Dict, Any, Tuple, Optional
import json
from pathlib import Path

from ..civitai_core.collector import CivitaiPromptCollector, CivitaiAPIClient
from ..civitai_core.database import DatabaseManager
from ..civitai_data_types import PromptDataset, PromptData
from .base_node import CivitaiNodeBase

class CivitaiDataCollectorNode(CivitaiNodeBase):
    """
    CivitAI APIã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒãƒ¼ãƒ‰

    æ©Ÿèƒ½:
    - ãƒ¢ãƒ‡ãƒ«ID/ãƒãƒ¼ã‚¸ãƒ§ãƒ³IDæŒ‡å®šã§ã®åé›†
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º
    - å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_id": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "ãƒ¢ãƒ‡ãƒ«ID (ä¾‹: 101055) ã¾ãŸã¯ç©ºæ–‡å­—"
                }),
                "version_id": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ID (ä¾‹: 128078) - æ¨å¥¨"
                }),
                "max_items": ("INT", {
                    "default": 100,
                    "min": 1,
                    "max": 5000,
                    "step": 1,
                    "tooltip": "åé›†ã™ã‚‹æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "CivitAI API Key (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ - åˆ¶é™ç·©å’Œ)"
                })
            },
            "optional": {
                "model_name": ("STRING", {
                    "default": "",
                    "placeholder": "ãƒ¢ãƒ‡ãƒ«åï¼ˆè¡¨ç¤ºç”¨ï¼‰"
                }),
                "strict_version_match": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å³å¯†ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆcheckpointã®ã¿ï¼‰"
                }),
                "quality_filter": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ä½å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é™¤å¤–"
                }),
                "min_reactions": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "tooltip": "æœ€å°ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°"
                }),
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨"
                })
            }
        }

    RETURN_TYPES = ("PROMPT_DATASET", "COLLECTION_STATS", "STRING")
    RETURN_NAMES = ("dataset", "stats", "summary")
    OUTPUT_NODE = False

    FUNCTION = "collect_prompts"
    CATEGORY = "CivitAI/Collection"

    DESCRIPTION = """
    CivitAI APIã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã™ã€‚

    ä½¿ç”¨æ–¹æ³•:
    1. version_id ã‚’æŒ‡å®šï¼ˆæ¨å¥¨ï¼‰
    2. max_items ã§åé›†æ•°ã‚’åˆ¶é™
    3. API Key ã§åˆ¶é™ç·©å’Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    æ³¨æ„: å¤§é‡åé›†ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™
    """

    def __init__(self):
        super().__init__()
        self.collector = None
        self.db_manager = None
        self._progress_callback = None

    def _init_components(self, api_key: str = ""):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é…å»¶åˆæœŸåŒ–"""
        if not self.collector:
            api_client = CivitaiAPIClient(api_key=api_key if api_key else None)
            self.collector = CivitaiPromptCollector(api_client)
        if not self.db_manager:
            self.db_manager = DatabaseManager()

    def _validate_inputs(self, model_id: str, version_id: str, max_items: int) -> Tuple[bool, str]:
        """å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼"""

        # å°‘ãªãã¨ã‚‚model_id ã¾ãŸã¯ version_id ãŒå¿…è¦
        if not model_id.strip() and not version_id.strip():
            return False, "model_id ã¾ãŸã¯ version_id ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"

        # max_items ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if not (1 <= max_items <= 5000):
            return False, f"max_items ã¯ 1-5000 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ (ç¾åœ¨: {max_items})"

        # version_id ã®å½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤æ¨å¥¨ï¼‰
        if version_id.strip() and not version_id.strip().isdigit():
            return True, f"è­¦å‘Š: version_id '{version_id}' ã¯æ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“"

        return True, "OK"

    def _estimate_collection_time(self, max_items: int) -> str:
        """åé›†æ™‚é–“ã®æ¦‚ç®—"""
        # APIåˆ¶é™ã‚’è€ƒæ…®ã—ãŸæ¦‚ç®—ï¼ˆ1.2ç§’/ãƒšãƒ¼ã‚¸, 100ã‚¢ã‚¤ãƒ†ãƒ /ãƒšãƒ¼ã‚¸ï¼‰
        pages = (max_items + 99) // 100
        estimated_seconds = pages * 1.2

        if estimated_seconds < 60:
            return f"ç´„ {int(estimated_seconds)} ç§’"
        else:
            return f"ç´„ {int(estimated_seconds / 60)} åˆ†"

    def collect_prompts(
        self,
        model_id: str,
        version_id: str,
        max_items: int,
        api_key: str,
        model_name: str = "",
        strict_version_match: bool = False,
        quality_filter: bool = True,
        min_reactions: int = 0,
        use_cache: bool = True
    ) -> Tuple[PromptDataset, Dict[str, Any], str]:
        """
        ãƒ¡ã‚¤ãƒ³ã®åé›†å‡¦ç†

        Returns:
            Tuple[PromptDataset, Dict[str, Any], str]: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ, çµ±è¨ˆæƒ…å ±, ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—
        """

        # 1. å…¥åŠ›æ¤œè¨¼
        is_valid, message = self._validate_inputs(model_id, version_id, max_items)
        if not is_valid:
            raise ValueError(message)

        # 2. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self._init_components(api_key)

        # 3. åé›†å‰ã®æº–å‚™
        target_id = version_id.strip() if version_id.strip() else model_id.strip()
        collection_target = "version" if version_id.strip() else "model"
        estimated_time = self._estimate_collection_time(max_items)

        print(f"[CivitAI] åé›†é–‹å§‹: {collection_target}={target_id}, max={max_items} ({estimated_time})")

        try:
            # 4. ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ
            collection_result = self.collector.collect_dataset(
                model_id=target_id,
                model_name=model_name if model_name else None,
                max_items=max_items
            )

            # 5. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            valid_prompts = []
            for prompt_data in collection_result["items"]:
                # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
                if quality_filter:
                    if prompt_data.get("quality_score", 0) < 5:
                        continue
                    if not prompt_data.get("full_prompt", "").strip():
                        continue

                # ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                if prompt_data.get("reaction_count", 0) < min_reactions:
                    continue

                # PromptData ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                prompt_obj = PromptData(
                    civitai_id=prompt_data.get("civitai_id", ""),
                    full_prompt=prompt_data.get("full_prompt", ""),
                    negative_prompt=prompt_data.get("negative_prompt", ""),
                    model_name=prompt_data.get("model_name", ""),
                    model_id=prompt_data.get("model_id", ""),
                    model_version_id=prompt_data.get("model_version_id", ""),
                    reaction_count=prompt_data.get("reaction_count", 0),
                    comment_count=prompt_data.get("comment_count", 0),
                    download_count=prompt_data.get("download_count", 0),
                    prompt_length=prompt_data.get("prompt_length", 0),
                    tag_count=prompt_data.get("tag_count", 0),
                    quality_score=prompt_data.get("quality_score", 0),
                    collected_at=prompt_data.get("collected_at", ""),
                    resources=prompt_data.get("resources", []),
                    raw_metadata=prompt_data.get("raw_metadata", "{}")
                )
                valid_prompts.append(prompt_obj)

            # 6. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
            dataset = PromptDataset(
                prompts=valid_prompts,
                metadata={
                    "collection_target": collection_target,
                    "target_id": target_id,
                    "model_name": model_name,
                    "collection_timestamp": collection_result.get("timestamp"),
                    "filters_applied": {
                        "quality_filter": quality_filter,
                        "min_reactions": min_reactions,
                        "strict_version_match": strict_version_match
                    }
                },
                collection_info={
                    "total_fetched": collection_result.get("collected", 0),
                    "total_valid": len(valid_prompts),
                    "estimated_time": estimated_time,
                    "actual_time": collection_result.get("elapsed_time"),
                    "api_source": "CivitAI v1"
                }
            )

            # 7. çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
            stats = self._generate_collection_stats(dataset, collection_result)

            # 8. ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—ç”Ÿæˆ
            summary = self._generate_summary(dataset, stats)

            print(f"[CivitAI] åé›†å®Œäº†: {len(valid_prompts)} ä»¶ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—")

            return (dataset, stats, summary)

        except Exception as e:
            error_msg = f"åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(f"[CivitAI] {error_msg}")

            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿”ã™
            empty_dataset = PromptDataset(
                prompts=[],
                metadata={"error": error_msg},
                collection_info={"status": "failed"}
            )
            error_stats = {"status": "error", "message": error_msg}

            return (empty_dataset, error_stats, error_msg)

    def _generate_collection_stats(self, dataset: PromptDataset, raw_result: Dict) -> Dict[str, Any]:
        """åé›†çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""

        prompts = dataset.prompts

        if not prompts:
            return {
                "total_prompts": 0,
                "status": "no_data"
            }

        # å“è³ªã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        quality_scores = [p.quality_score for p in prompts]

        # ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ
        models = {}
        for p in prompts:
            model = p.model_name or "Unknown"
            models[model] = models.get(model, 0) + 1

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·åˆ†å¸ƒ
        lengths = [p.prompt_length for p in prompts]

        return {
            "total_prompts": len(prompts),
            "quality_stats": {
                "min": min(quality_scores),
                "max": max(quality_scores),
                "avg": sum(quality_scores) / len(quality_scores)
            },
            "length_stats": {
                "min": min(lengths),
                "max": max(lengths),
                "avg": sum(lengths) / len(lengths)
            },
            "model_distribution": models,
            "collection_efficiency": len(prompts) / raw_result.get("collected", 1),
            "status": "success"
        }

    def _generate_summary(self, dataset: PromptDataset, stats: Dict[str, Any]) -> str:
        """äººé–“å‘ã‘ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—ã‚’ç”Ÿæˆ"""

        if stats.get("status") == "error":
            return f"âŒ åé›†å¤±æ•—: {stats.get('message', 'Unknown error')}"

        if stats.get("status") == "no_data":
            return "âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"

        total = stats.get("total_prompts", 0)
        avg_quality = stats.get("quality_stats", {}).get("avg", 0)
        models = len(stats.get("model_distribution", {}))

        summary = f"âœ… åé›†å®Œäº†: {total} ä»¶ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n"
        summary += f"ğŸ“Š å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.1f}\n"
        summary += f"ğŸ¨ ãƒ¢ãƒ‡ãƒ«æ•°: {models}\n"

        if dataset.metadata.get("filters_applied"):
            filters = dataset.metadata["filters_applied"]
            if filters.get("quality_filter"):
                summary += "ğŸ” å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: æœ‰åŠ¹\n"
            if filters.get("min_reactions", 0) > 0:
                summary += f"ğŸ‘ æœ€å°ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {filters['min_reactions']}\n"

        return summary.strip()

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡ - å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤‰ã‚ã£ãŸå ´åˆã®ã¿å†å®Ÿè¡Œ"""
        cache_key = f"{kwargs.get('model_id', '')}_{kwargs.get('version_id', '')}_{kwargs.get('max_items', 0)}"
        return hash(cache_key)


# ä½¿ç”¨ä¾‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ JSON
EXAMPLE_WORKFLOW = {
    "nodes": {
        "1": {
            "id": 1,
            "type": "CivitaiDataCollector",
            "pos": [100, 100],
            "size": [400, 300],
            "flags": {},
            "order": 0,
            "mode": 0,
            "inputs": [],
            "outputs": [
                {"name": "dataset", "type": "PROMPT_DATASET", "links": [1]},
                {"name": "stats", "type": "COLLECTION_STATS", "links": [2]},
                {"name": "summary", "type": "STRING", "links": [3]}
            ],
            "properties": {},
            "widgets_values": [
                "",        # model_id
                "128078",  # version_id
                100,       # max_items
                "",        # api_key
                "Realistic Vision",  # model_name
                False,     # strict_version_match
                True,      # quality_filter
                5,         # min_reactions
                True       # use_cache
            ]
        }
    },
    "links": [
        [1, 1, 0, 2, 0, "PROMPT_DATASET"],
        [2, 1, 1, 3, 0, "COLLECTION_STATS"],
        [3, 1, 2, 4, 0, "STRING"]
    ],
    "groups": [],
    "config": {},
    "extra": {},
    "version": 0.4
}
```

## 2. ãƒ‡ãƒ¼ã‚¿å‹ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…

```python
# civitai_data_types.py

import json
import pickle
import base64
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass, asdict, field
from datetime import datetime

@dataclass
class PromptData:
    """å€‹åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ - ComfyUIé–“ã®åŠ¹ç‡çš„ãªè»¢é€ã‚’è€ƒæ…®"""

    # Core identification
    civitai_id: str
    full_prompt: str
    negative_prompt: str = ""

    # Model information
    model_name: str = ""
    model_id: str = ""
    model_version_id: str = ""

    # Engagement metrics
    reaction_count: int = 0
    comment_count: int = 0
    download_count: int = 0

    # Derived metrics
    prompt_length: int = 0
    tag_count: int = 0
    quality_score: int = 0

    # Metadata
    collected_at: str = ""
    resources: List[Dict[str, Any]] = field(default_factory=list)
    raw_metadata: str = "{}"

    def __post_init__(self):
        """è‡ªå‹•è¨ˆç®—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®è¨­å®š"""
        if not self.prompt_length:
            self.prompt_length = len(self.full_prompt)
        if not self.tag_count and self.full_prompt:
            self.tag_count = len([t for t in self.full_prompt.split(',') if t.strip()])

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆJSON serializableï¼‰"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PromptData':
        """è¾æ›¸ã‹ã‚‰å¾©å…ƒ"""
        return cls(**data)

    def get_tags(self) -> List[str]:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã‚¿ã‚°ãƒªã‚¹ãƒˆã‚’æŠ½å‡º"""
        return [tag.strip() for tag in self.full_prompt.split(',') if tag.strip()]

    def matches_keywords(self, keywords: List[str]) -> bool:
        """æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®ãƒãƒƒãƒãƒ³ã‚°åˆ¤å®š"""
        text = (self.full_prompt + " " + self.negative_prompt).lower()
        return any(keyword.lower() in text for keyword in keywords)


class PromptDataset:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ - ComfyUIã§ã®åŠ¹ç‡çš„ãªå‡¦ç†ã‚’è€ƒæ…®"""

    def __init__(
        self,
        prompts: List[PromptData] = None,
        metadata: Dict[str, Any] = None,
        collection_info: Dict[str, Any] = None
    ):
        self.prompts = prompts or []
        self.metadata = metadata or {}
        self.collection_info = collection_info or {}
        self._cache = {}  # è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥

    def __len__(self) -> int:
        return len(self.prompts)

    def __iter__(self):
        return iter(self.prompts)

    def __getitem__(self, index: Union[int, slice]) -> Union[PromptData, 'PromptDataset']:
        if isinstance(index, slice):
            return PromptDataset(
                prompts=self.prompts[index],
                metadata=self.metadata.copy(),
                collection_info=self.collection_info.copy()
            )
        return self.prompts[index]

    # ComfyUI ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
    def serialize(self) -> str:
        """ComfyUIé–“è»¢é€ç”¨ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"""
        data = {
            'prompts': [p.to_dict() for p in self.prompts],
            'metadata': self.metadata,
            'collection_info': self.collection_info,
            'version': '1.0'
        }
        return base64.b64encode(json.dumps(data, ensure_ascii=False).encode('utf-8')).decode('ascii')

    @classmethod
    def deserialize(cls, data_str: str) -> 'PromptDataset':
        """ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¾©å…ƒ"""
        try:
            json_str = base64.b64decode(data_str.encode('ascii')).decode('utf-8')
            data = json.loads(json_str)

            prompts = [PromptData.from_dict(p) for p in data.get('prompts', [])]
            return cls(
                prompts=prompts,
                metadata=data.get('metadata', {}),
                collection_info=data.get('collection_info', {})
            )
        except Exception as e:
            print(f"[CivitAI] ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return cls()  # ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¿”å´

    # ãƒ‡ãƒ¼ã‚¿æ“ä½œãƒ¡ã‚½ãƒƒãƒ‰
    def filter_by_quality(self, min_score: int) -> 'PromptDataset':
        """å“è³ªã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_prompts = [p for p in self.prompts if p.quality_score >= min_score]
        return PromptDataset(
            prompts=filtered_prompts,
            metadata=self._add_filter_metadata("quality_filter", {"min_score": min_score}),
            collection_info=self.collection_info.copy()
        )

    def filter_by_reactions(self, min_reactions: int) -> 'PromptDataset':
        """ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        filtered_prompts = [p for p in self.prompts if p.reaction_count >= min_reactions]
        return PromptDataset(
            prompts=filtered_prompts,
            metadata=self._add_filter_metadata("reaction_filter", {"min_reactions": min_reactions}),
            collection_info=self.collection_info.copy()
        )

    def filter_by_keywords(self, keywords: List[str], exclude: bool = False) -> 'PromptDataset':
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if exclude:
            filtered_prompts = [p for p in self.prompts if not p.matches_keywords(keywords)]
        else:
            filtered_prompts = [p for p in self.prompts if p.matches_keywords(keywords)]

        return PromptDataset(
            prompts=filtered_prompts,
            metadata=self._add_filter_metadata("keyword_filter", {"keywords": keywords, "exclude": exclude}),
            collection_info=self.collection_info.copy()
        )

    def sample(self, count: int, method: str = "random", seed: int = None) -> 'PromptDataset':
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        import random

        if seed is not None:
            random.seed(seed)

        if method == "random":
            sampled = random.sample(self.prompts, min(count, len(self.prompts)))
        elif method == "top_quality":
            sampled = sorted(self.prompts, key=lambda p: p.quality_score, reverse=True)[:count]
        elif method == "top_reactions":
            sampled = sorted(self.prompts, key=lambda p: p.reaction_count, reverse=True)[:count]
        else:
            sampled = self.prompts[:count]

        return PromptDataset(
            prompts=sampled,
            metadata=self._add_filter_metadata("sample", {"count": count, "method": method, "seed": seed}),
            collection_info=self.collection_info.copy()
        )

    def get_statistics(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        cache_key = "statistics"
        if cache_key in self._cache:
            return self._cache[cache_key]

        if not self.prompts:
            stats = {"total": 0, "status": "empty"}
        else:
            quality_scores = [p.quality_score for p in self.prompts]
            reaction_counts = [p.reaction_count for p in self.prompts]
            prompt_lengths = [p.prompt_length for p in self.prompts]

            # ãƒ¢ãƒ‡ãƒ«åˆ†å¸ƒ
            model_dist = {}
            for p in self.prompts:
                model = p.model_name or "Unknown"
                model_dist[model] = model_dist.get(model, 0) + 1

            stats = {
                "total": len(self.prompts),
                "quality_stats": {
                    "min": min(quality_scores),
                    "max": max(quality_scores),
                    "avg": sum(quality_scores) / len(quality_scores),
                    "distribution": self._get_distribution(quality_scores, bins=10)
                },
                "reaction_stats": {
                    "min": min(reaction_counts),
                    "max": max(reaction_counts),
                    "avg": sum(reaction_counts) / len(reaction_counts),
                    "distribution": self._get_distribution(reaction_counts, bins=10)
                },
                "length_stats": {
                    "min": min(prompt_lengths),
                    "max": max(prompt_lengths),
                    "avg": sum(prompt_lengths) / len(prompt_lengths)
                },
                "model_distribution": model_dist,
                "status": "complete"
            }

        self._cache[cache_key] = stats
        return stats

    def _add_filter_metadata(self, filter_name: str, filter_params: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨å±¥æ­´ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ """
        new_metadata = self.metadata.copy()
        filters = new_metadata.setdefault("applied_filters", [])
        filters.append({
            "filter": filter_name,
            "params": filter_params,
            "timestamp": datetime.now().isoformat()
        })
        return new_metadata

    def _get_distribution(self, values: List[int], bins: int = 10) -> Dict[str, int]:
        """å€¤ã®åˆ†å¸ƒã‚’è¨ˆç®—"""
        if not values:
            return {}

        min_val, max_val = min(values), max(values)
        if min_val == max_val:
            return {str(min_val): len(values)}

        bin_size = (max_val - min_val) / bins
        distribution = {}

        for i in range(bins):
            bin_min = min_val + i * bin_size
            bin_max = min_val + (i + 1) * bin_size
            bin_key = f"{int(bin_min)}-{int(bin_max)}"

            if i == bins - 1:  # æœ€å¾Œã®ãƒ“ãƒ³
                count = len([v for v in values if bin_min <= v <= bin_max])
            else:
                count = len([v for v in values if bin_min <= v < bin_max])

            if count > 0:
                distribution[bin_key] = count

        return distribution


# ComfyUI ã‚«ã‚¹ã‚¿ãƒ å‹ç™»éŒ²
def register_civitai_types():
    """ComfyUIã«ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç™»éŒ²"""

    try:
        # ComfyUI ã®å‹ã‚·ã‚¹ãƒ†ãƒ ã«ç™»éŒ²
        import comfy.model_management as mm

        # PROMPT_DATASET å‹ã®å®šç¾©
        class PromptDatasetType:
            @staticmethod
            def serialize(obj: PromptDataset) -> str:
                return obj.serialize()

            @staticmethod
            def deserialize(data: str) -> PromptDataset:
                return PromptDataset.deserialize(data)

            @staticmethod
            def validate(obj) -> bool:
                return isinstance(obj, PromptDataset)

        # å‹ç™»éŒ²
        mm.register_custom_type("PROMPT_DATASET", PromptDatasetType)
        mm.register_custom_type("CLASSIFIED_DATASET", PromptDatasetType)  # åŒã˜åŸºåº•ã‚¯ãƒ©ã‚¹
        mm.register_custom_type("FILTERED_DATASET", PromptDatasetType)   # åŒã˜åŸºåº•ã‚¯ãƒ©ã‚¹

        print("[CivitAI] ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")

    except ImportError:
        print("[CivitAI] è­¦å‘Š: ComfyUIç’°å¢ƒå¤–ã§ã®å®Ÿè¡Œ - å‹ç™»éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    except Exception as e:
        print(f"[CivitAI] å‹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")


# å‹ç™»éŒ²ã‚’å®Ÿè¡Œ
register_civitai_types()
```

## 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ãƒã‚¹ãƒˆãƒã‚¹å®Ÿè£…

```python
# utils/error_handler.py

import functools
import time
import logging
from typing import Any, Callable, Optional, Dict
from enum import Enum

class CivitaiErrorType(Enum):
    """ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥"""
    API_RATE_LIMIT = "api_rate_limit"
    API_TIMEOUT = "api_timeout"
    API_AUTH = "api_auth"
    API_NOT_FOUND = "api_not_found"
    DATA_INVALID = "data_invalid"
    NETWORK_ERROR = "network_error"
    UNKNOWN_ERROR = "unknown_error"

class CivitaiError(Exception):
    """CivitAIé–¢é€£ã‚¨ãƒ©ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, message: str, error_type: CivitaiErrorType, details: Dict[str, Any] = None):
        super().__init__(message)
        self.error_type = error_type
        self.details = details or {}
        self.timestamp = time.time()

class APIRateLimitError(CivitaiError):
    """APIåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str = "API rate limit exceeded", retry_after: int = None):
        super().__init__(message, CivitaiErrorType.API_RATE_LIMIT, {"retry_after": retry_after})

class APITimeoutError(CivitaiError):
    """API timeout ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str = "API request timed out", timeout: float = None):
        super().__init__(message, CivitaiErrorType.API_TIMEOUT, {"timeout": timeout})

class DataValidationError(CivitaiError):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str, field: str = None, value: Any = None):
        super().__init__(message, CivitaiErrorType.DATA_INVALID, {"field": field, "value": value})

def with_retry(
    max_retries: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ããƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e

                    if attempt == max_retries:
                        # æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—
                        break

                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆã¯å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´
                    if isinstance(e, APIRateLimitError):
                        wait_time = e.details.get("retry_after", delay * (backoff ** attempt))
                    else:
                        wait_time = delay * (backoff ** attempt)

                    print(f"[Retry] Attempt {attempt + 1}/{max_retries + 1} failed: {e}")
                    print(f"[Retry] Waiting {wait_time:.1f}s before retry...")
                    time.sleep(wait_time)

            # ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—
            raise last_exception

        return wrapper
    return decorator

def safe_execute(default_return: Any = None, log_errors: bool = True):
    """å®‰å…¨å®Ÿè¡Œãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ - ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™"""

    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if log_errors:
                    logging.error(f"Error in {func.__name__}: {e}", exc_info=True)

                # ComfyUI ãƒãƒ¼ãƒ‰ç”¨ã®ã‚¨ãƒ©ãƒ¼æˆ»ã‚Šå€¤
                if isinstance(default_return, tuple):
                    return default_return
                else:
                    return (default_return, {"error": str(e), "function": func.__name__})

        return wrapper
    return decorator

class CivitaiErrorHandler:
    """çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self):
        self.error_history = []
        self.max_history = 100

    def handle_api_error(self, response_code: int, response_text: str = "") -> CivitaiError:
        """API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†"""

        if response_code == 429:
            # Rate limit
            retry_after = self._extract_retry_after(response_text)
            error = APIRateLimitError(retry_after=retry_after)
        elif response_code == 401:
            # Authentication
            error = CivitaiError("API authentication failed", CivitaiErrorType.API_AUTH)
        elif response_code == 404:
            # Not found
            error = CivitaiError("Resource not found", CivitaiErrorType.API_NOT_FOUND)
        elif response_code >= 500:
            # Server error
            error = CivitaiError(f"Server error: {response_code}", CivitaiErrorType.API_TIMEOUT)
        else:
            # Unknown error
            error = CivitaiError(f"API error: {response_code}", CivitaiErrorType.UNKNOWN_ERROR)

        self._log_error(error)
        return error

    def handle_network_error(self, original_error: Exception) -> CivitaiError:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†"""

        error_msg = str(original_error)

        if "timeout" in error_msg.lower():
            error = APITimeoutError()
        elif "connection" in error_msg.lower():
            error = CivitaiError("Network connection error", CivitaiErrorType.NETWORK_ERROR)
        else:
            error = CivitaiError(f"Network error: {error_msg}", CivitaiErrorType.NETWORK_ERROR)

        self._log_error(error)
        return error

    def validate_input(self, value: Any, field_name: str, validator: Callable[[Any], bool], error_message: str = None) -> None:
        """å…¥åŠ›å€¤æ¤œè¨¼"""

        if not validator(value):
            message = error_message or f"Invalid value for {field_name}: {value}"
            raise DataValidationError(message, field=field_name, value=value)

    def get_user_friendly_message(self, error: CivitaiError) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""

        if error.error_type == CivitaiErrorType.API_RATE_LIMIT:
            retry_after = error.details.get("retry_after")
            if retry_after:
                return f"â³ APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{retry_after}ç§’å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
            else:
                return "â³ APIåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"

        elif error.error_type == CivitaiErrorType.API_AUTH:
            return "ğŸ”‘ APIèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        elif error.error_type == CivitaiErrorType.API_NOT_FOUND:
            return "â“ æŒ‡å®šã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚IDã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        elif error.error_type == CivitaiErrorType.API_TIMEOUT:
            return "â±ï¸ APIæ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        elif error.error_type == CivitaiErrorType.DATA_INVALID:
            field = error.details.get("field", "ä¸æ˜ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰")
            return f"ğŸ“ å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {field} ã®å€¤ãŒç„¡åŠ¹ã§ã™ã€‚"

        elif error.error_type == CivitaiErrorType.NETWORK_ERROR:
            return "ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        else:
            return f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(error)}"

    def _extract_retry_after(self, response_text: str) -> Optional[int]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ retry-after å€¤ã‚’æŠ½å‡º"""
        # å®Ÿè£…: ãƒ˜ãƒƒãƒ€ãƒ¼ã¾ãŸã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã‹ã‚‰æŠ½å‡º
        return None

    def _log_error(self, error: CivitaiError) -> None:
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²"""
        self.error_history.append({
            "error": error,
            "timestamp": error.timestamp,
            "type": error.error_type.value,
            "message": str(error)
        })

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.error_history) > self.max_history:
            self.error_history = self.error_history[-self.max_history:]

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
error_handler = CivitaiErrorHandler()

# ä¾¿åˆ©ãªãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def robust_api_call(func: Callable) -> Callable:
    """API ã‚³ãƒ¼ãƒ«ç”¨ã®ãƒ­ãƒã‚¹ãƒˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @with_retry(max_retries=3, delay=2.0, exceptions=(APIRateLimitError, APITimeoutError))
    @safe_execute(default_return=(None, {"error": "API call failed"}))
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        return func(*args, **kwargs)

    return wrapper
```

ã“ã®æŠ€è¡“ä»•æ§˜æ›¸ã«ã‚ˆã‚Šã€ç¾åœ¨ã®CivitAI Prompt Collectorã‚’åŠ¹ç‡çš„ã‹ã¤å …ç‰¢ãªComfyUIãƒãƒ¼ãƒ‰ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ãŒæä¾›ã•ã‚Œã¾ã™ã€‚å®Ÿè£…æ™‚ã¯ã€æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šã€å„ãƒ•ã‚§ãƒ¼ã‚ºã§ãƒ†ã‚¹ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é‡è¦–ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚
